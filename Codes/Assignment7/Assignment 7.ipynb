{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1095e2",
   "metadata": {},
   "source": [
    "### Assignment 7\n",
    "#### Name: Sourabh Kumbhar\n",
    "#### Roll No: 31147\n",
    "#### Batch: M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bf0ff",
   "metadata": {},
   "source": [
    "##### Problem Statement\n",
    "Text Analytics\n",
    "    1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "    2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f60d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42bda68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/sourabh/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff6c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millions of people in India took part in an annual tree planting drive Sunday. More than 250 million saplings were planted in a single day across the country's most-populous state.The campaign was led by Uttar Pradesh state government officials, lawmakers, and activists, in a bid to reduce carbon emissions and combat climate change.Where were the trees planted?The saplings were planted by volunteers in forests, farms, schools, and along riverbanks and highways.\"We are committed to increasing the forest cover of Uttar Pradesh to over 15% of the total land area in the next five years,'' said state forest official Manoj Singh.According to another government official, the forest cover of the state has increased over the last few years.\"There has been an increase of 127 sqare kilometers [79 sqare miles] in the forest cover in Uttar Pradesh as compared to 2017,\" a state government spokesperson was quoted as saying in The Indian Express newspaper.\"There has also been an increase in trees and plants. The tree cover has increased to 3.05%, as compared to the national average of 2.89%,\" the official said, citing the 2019 Forest Survey of India report.How many saplings survive?Uttar Pradesh State Forest Minister Dara Singh said the long-term survival of the saplings remains a concern, adding that usually only 60% of the saplings survive. The rest succumb to disease or lack of water.However, he said that about 80% of the saplings planted in the last four annual drives have survived.\"All the regions where plantation is being carried out have been geo-tagged so that we can ascertain what exactly happened,\" Chauhan told The Pioneer newspaper.\"These saplings carry QR codes so that officials can maintain a record and verify whether the saplings survived or not. Besides, teams have been formed to monitor progress of the plantation drive,\" he said.What is the extent of India's tree planting project?India has vowed to have a third of its total land area, or 95 million hectares, under forest and tree cover by 2030.The government has allocated $6.2 billion (€5.2 billion) for the tree-planting across the country.However, industrial development and a rapidly growing population has put further stress on the land.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open('testdoc.txt') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    text += line\n",
    "text = text.replace('\\n','')\n",
    "print(text)\n",
    "# text = \"It was July 21, 1969, and Neil Armstrong awoke with a start. It was the day he would become the first human being to ever walk on the moon. The journey had begun several days earlier, when on July 16th, the Apollo 11 launched from Earth headed into outer space. On board with Neil Armstrong were Michael Collins and Buzz Aldrin. The crew landed on the moon in the Sea of Tranquility a day before the actual walk. Upon Neil’s first step onto the moon’s surface, he declared, “That’s one small step for man, one giant leap for mankind.” It sure was!\"\n",
    "# text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2c14a",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc6cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions of people in India took part in an annual tree planting drive Sunday.', \"More than 250 million saplings were planted in a single day across the country's most-populous state.The campaign was led by Uttar Pradesh state government officials, lawmakers, and activists, in a bid to reduce carbon emissions and combat climate change.Where were the trees planted?The saplings were planted by volunteers in forests, farms, schools, and along riverbanks and highways.\", '\"We are committed to increasing the forest cover of Uttar Pradesh to over 15% of the total land area in the next five years,\\'\\' said state forest official Manoj Singh.According to another government official, the forest cover of the state has increased over the last few years.', '\"There has been an increase of 127 sqare kilometers [79 sqare miles]\\xa0in the forest cover in Uttar Pradesh as compared to 2017,\" a state government spokesperson was quoted as saying in\\xa0The Indian Express newspaper.', '\"There has also been an increase in trees and plants.', 'The tree cover has increased to 3.05%, as compared to the national average of 2.89%,\" the official said, citing the 2019 Forest Survey of India report.How many saplings survive?Uttar Pradesh State Forest Minister Dara Singh said the long-term survival of the saplings remains a concern, adding that usually only 60% of the saplings survive.', 'The rest succumb to disease or lack of water.However, he said that\\xa0about 80% of the saplings planted in the last four annual drives have survived.', '\"All the regions where plantation is being carried out have been geo-tagged so that we can ascertain what exactly happened,\" Chauhan told The Pioneer newspaper.', '\"These saplings carry QR codes so that officials can maintain a record and verify whether the saplings survived or not.', 'Besides, teams have been formed to monitor progress of the plantation drive,\" he said.What is the extent of India\\'s tree planting project?India has vowed to have a third of its total land area, or 95 million hectares, under forest and tree cover by 2030.The government has allocated $6.2 billion (€5.2 billion) for the tree-planting across the country.However, industrial development and a rapidly growing population has put further stress on the land.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87981a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'of', 'people', 'in', 'India', 'took', 'part', 'in', 'an', 'annual', 'tree', 'planting', 'drive', 'Sunday', '.', 'More', 'than', '250', 'million', 'saplings', 'were', 'planted', 'in', 'a', 'single', 'day', 'across', 'the', 'country', \"'s\", 'most-populous', 'state.The', 'campaign', 'was', 'led', 'by', 'Uttar', 'Pradesh', 'state', 'government', 'officials', ',', 'lawmakers', ',', 'and', 'activists', ',', 'in', 'a', 'bid', 'to', 'reduce', 'carbon', 'emissions', 'and', 'combat', 'climate', 'change.Where', 'were', 'the', 'trees', 'planted', '?', 'The', 'saplings', 'were', 'planted', 'by', 'volunteers', 'in', 'forests', ',', 'farms', ',', 'schools', ',', 'and', 'along', 'riverbanks', 'and', 'highways', '.', '``', 'We', 'are', 'committed', 'to', 'increasing', 'the', 'forest', 'cover', 'of', 'Uttar', 'Pradesh', 'to', 'over', '15', '%', 'of', 'the', 'total', 'land', 'area', 'in', 'the', 'next', 'five', 'years', ',', \"''\", 'said', 'state', 'forest', 'official', 'Manoj', 'Singh.According', 'to', 'another', 'government', 'official', ',', 'the', 'forest', 'cover', 'of', 'the', 'state', 'has', 'increased', 'over', 'the', 'last', 'few', 'years', '.', '``', 'There', 'has', 'been', 'an', 'increase', 'of', '127', 'sqare', 'kilometers', '[', '79', 'sqare', 'miles', ']', 'in', 'the', 'forest', 'cover', 'in', 'Uttar', 'Pradesh', 'as', 'compared', 'to', '2017', ',', \"''\", 'a', 'state', 'government', 'spokesperson', 'was', 'quoted', 'as', 'saying', 'in', 'The', 'Indian', 'Express', 'newspaper', '.', '``', 'There', 'has', 'also', 'been', 'an', 'increase', 'in', 'trees', 'and', 'plants', '.', 'The', 'tree', 'cover', 'has', 'increased', 'to', '3.05', '%', ',', 'as', 'compared', 'to', 'the', 'national', 'average', 'of', '2.89', '%', ',', \"''\", 'the', 'official', 'said', ',', 'citing', 'the', '2019', 'Forest', 'Survey', 'of', 'India', 'report.How', 'many', 'saplings', 'survive', '?', 'Uttar', 'Pradesh', 'State', 'Forest', 'Minister', 'Dara', 'Singh', 'said', 'the', 'long-term', 'survival', 'of', 'the', 'saplings', 'remains', 'a', 'concern', ',', 'adding', 'that', 'usually', 'only', '60', '%', 'of', 'the', 'saplings', 'survive', '.', 'The', 'rest', 'succumb', 'to', 'disease', 'or', 'lack', 'of', 'water.However', ',', 'he', 'said', 'that', 'about', '80', '%', 'of', 'the', 'saplings', 'planted', 'in', 'the', 'last', 'four', 'annual', 'drives', 'have', 'survived', '.', '``', 'All', 'the', 'regions', 'where', 'plantation', 'is', 'being', 'carried', 'out', 'have', 'been', 'geo-tagged', 'so', 'that', 'we', 'can', 'ascertain', 'what', 'exactly', 'happened', ',', \"''\", 'Chauhan', 'told', 'The', 'Pioneer', 'newspaper', '.', '``', 'These', 'saplings', 'carry', 'QR', 'codes', 'so', 'that', 'officials', 'can', 'maintain', 'a', 'record', 'and', 'verify', 'whether', 'the', 'saplings', 'survived', 'or', 'not', '.', 'Besides', ',', 'teams', 'have', 'been', 'formed', 'to', 'monitor', 'progress', 'of', 'the', 'plantation', 'drive', ',', \"''\", 'he', 'said.What', 'is', 'the', 'extent', 'of', 'India', \"'s\", 'tree', 'planting', 'project', '?', 'India', 'has', 'vowed', 'to', 'have', 'a', 'third', 'of', 'its', 'total', 'land', 'area', ',', 'or', '95', 'million', 'hectares', ',', 'under', 'forest', 'and', 'tree', 'cover', 'by', '2030.The', 'government', 'has', 'allocated', '$', '6.2', 'billion', '(', '€5.2', 'billion', ')', 'for', 'the', 'tree-planting', 'across', 'the', 'country.However', ',', 'industrial', 'development', 'and', 'a', 'rapidly', 'growing', 'population', 'has', 'put', 'further', 'stress', 'on', 'the', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdc2a8",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55708d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sourabh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sourabh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb12bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Millions', 'NNS'), ('people', 'NNS'), ('India', 'NNP'), ('took', 'VBD'), ('part', 'NN'), ('annual', 'JJ'), ('tree', 'NN'), ('planting', 'VBG'), ('drive', 'JJ'), ('Sunday', 'NNP'), ('.', '.')]\n",
      "[('More', 'RBR'), ('250', 'CD'), ('million', 'CD'), ('saplings', 'NNS'), ('planted', 'VBN'), ('single', 'JJ'), ('day', 'NN'), ('across', 'IN'), ('country', 'NN'), (\"'s\", 'POS'), ('most-populous', 'JJ'), ('state.The', 'NN'), ('campaign', 'NN'), ('led', 'VBD'), ('Uttar', 'NNP'), ('Pradesh', 'NNP'), ('state', 'NN'), ('government', 'NN'), ('officials', 'NNS'), (',', ','), ('lawmakers', 'NNS'), (',', ','), ('activists', 'NNS'), (',', ','), ('bid', 'NN'), ('reduce', 'VB'), ('carbon', 'NN'), ('emissions', 'NNS'), ('combat', 'VBP'), ('climate', 'NN'), ('change.Where', 'NN'), ('trees', 'NNS'), ('planted', 'VBN'), ('?', '.'), ('The', 'DT'), ('saplings', 'NNS'), ('planted', 'VBD'), ('volunteers', 'NNS'), ('forests', 'NNS'), (',', ','), ('farms', 'NNS'), (',', ','), ('schools', 'NNS'), (',', ','), ('along', 'IN'), ('riverbanks', 'NNS'), ('highways', 'NNS'), ('.', '.')]\n",
      "[('``', '``'), ('We', 'PRP'), ('committed', 'VBD'), ('increasing', 'VBG'), ('forest', 'JJS'), ('cover', 'NN'), ('Uttar', 'NNP'), ('Pradesh', 'NNP'), ('15', 'CD'), ('%', 'NN'), ('total', 'JJ'), ('land', 'NN'), ('area', 'NN'), ('next', 'IN'), ('five', 'CD'), ('years', 'NNS'), (',', ','), (\"''\", \"''\"), ('said', 'VBD'), ('state', 'NN'), ('forest', 'JJS'), ('official', 'NN'), ('Manoj', 'NNP'), ('Singh.According', 'NNP'), ('another', 'DT'), ('government', 'NN'), ('official', 'NN'), (',', ','), ('forest', 'JJS'), ('cover', 'NN'), ('state', 'NN'), ('increased', 'VBD'), ('last', 'JJ'), ('years', 'NNS'), ('.', '.')]\n",
      "[('``', '``'), ('There', 'EX'), ('increase', 'VB'), ('127', 'CD'), ('sqare', 'JJ'), ('kilometers', 'NNS'), ('[', 'VBP'), ('79', 'CD'), ('sqare', 'NN'), ('miles', 'NNS'), (']', 'RB'), ('forest', 'VBP'), ('cover', 'NN'), ('Uttar', 'NNP'), ('Pradesh', 'NNP'), ('compared', 'VBN'), ('2017', 'CD'), (',', ','), (\"''\", \"''\"), ('state', 'NN'), ('government', 'NN'), ('spokesperson', 'NN'), ('quoted', 'VBD'), ('saying', 'VBG'), ('The', 'DT'), ('Indian', 'NNP'), ('Express', 'NNP'), ('newspaper', 'NN'), ('.', '.')]\n",
      "[('``', '``'), ('There', 'EX'), ('also', 'RB'), ('increase', 'NN'), ('trees', 'NNS'), ('plants', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('tree', 'JJ'), ('cover', 'NN'), ('increased', 'VBD'), ('3.05', 'CD'), ('%', 'NN'), (',', ','), ('compared', 'VBN'), ('national', 'JJ'), ('average', 'JJ'), ('2.89', 'CD'), ('%', 'NN'), (',', ','), (\"''\", \"''\"), ('official', 'NN'), ('said', 'VBD'), (',', ','), ('citing', 'VBG'), ('2019', 'CD'), ('Forest', 'NNP'), ('Survey', 'NNP'), ('India', 'NNP'), ('report.How', 'VBD'), ('many', 'JJ'), ('saplings', 'NNS'), ('survive', 'VBP'), ('?', '.'), ('Uttar', 'NNP'), ('Pradesh', 'NNP'), ('State', 'NNP'), ('Forest', 'NNP'), ('Minister', 'NNP'), ('Dara', 'NNP'), ('Singh', 'NNP'), ('said', 'VBD'), ('long-term', 'JJ'), ('survival', 'JJ'), ('saplings', 'NNS'), ('remains', 'VBZ'), ('concern', 'NN'), (',', ','), ('adding', 'VBG'), ('usually', 'RB'), ('60', 'CD'), ('%', 'NN'), ('saplings', 'NNS'), ('survive', 'VBP'), ('.', '.')]\n",
      "[('The', 'DT'), ('rest', 'NN'), ('succumb', 'NN'), ('disease', 'NN'), ('lack', 'NN'), ('water.However', 'NN'), (',', ','), ('said', 'VBD'), ('80', 'CD'), ('%', 'NN'), ('saplings', 'NNS'), ('planted', 'VBN'), ('last', 'JJ'), ('four', 'CD'), ('annual', 'JJ'), ('drives', 'NNS'), ('survived', 'VBD'), ('.', '.')]\n",
      "[('``', '``'), ('All', 'DT'), ('regions', 'NNS'), ('plantation', 'VBP'), ('carried', 'VBN'), ('geo-tagged', 'JJ'), ('ascertain', 'NN'), ('exactly', 'RB'), ('happened', 'VBD'), (',', ','), (\"''\", \"''\"), ('Chauhan', 'NNP'), ('told', 'VBD'), ('The', 'DT'), ('Pioneer', 'NNP'), ('newspaper', 'NN'), ('.', '.')]\n",
      "[('``', '``'), ('These', 'DT'), ('saplings', 'NNS'), ('carry', 'VBP'), ('QR', 'NNP'), ('codes', 'NNS'), ('officials', 'NNS'), ('maintain', 'VBP'), ('record', 'NN'), ('verify', 'NN'), ('whether', 'IN'), ('saplings', 'NNS'), ('survived', 'VBN'), ('.', '.')]\n",
      "[('Besides', 'IN'), (',', ','), ('teams', 'NNS'), ('formed', 'VBD'), ('monitor', 'JJ'), ('progress', 'JJ'), ('plantation', 'NN'), ('drive', 'NN'), (',', ','), (\"''\", \"''\"), ('said.What', 'WDT'), ('extent', 'VBD'), ('India', 'NNP'), (\"'s\", 'POS'), ('tree', 'NN'), ('planting', 'NN'), ('project', 'NN'), ('?', '.'), ('India', 'NNP'), ('vowed', 'VBD'), ('third', 'JJ'), ('total', 'JJ'), ('land', 'NN'), ('area', 'NN'), (',', ','), ('95', 'CD'), ('million', 'CD'), ('hectares', 'NNS'), (',', ','), ('forest', 'JJS'), ('tree', 'NN'), ('cover', 'NN'), ('2030.The', 'CD'), ('government', 'NN'), ('allocated', 'VBD'), ('$', '$'), ('6.2', 'CD'), ('billion', 'CD'), ('(', '('), ('€5.2', 'NNP'), ('billion', 'CD'), (')', ')'), ('tree-planting', 'NN'), ('across', 'IN'), ('country.However', 'NN'), (',', ','), ('industrial', 'JJ'), ('development', 'NN'), ('rapidly', 'RB'), ('growing', 'VBG'), ('population', 'NN'), ('put', 'VBD'), ('stress', 'JJ'), ('land', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokenized = sent_tokenize(text)\n",
    "for i in tokenized:\n",
    "  # Word tokenizers is used to find the words\n",
    "  # and punctuation in a string\n",
    "  wordsList = nltk.word_tokenize(i)\n",
    "  # removing stop words from wordList\n",
    "  wordsList = [w for w in wordsList if not w in stop_words]\n",
    "  # Using a Tagger. Which is part-of-speech\n",
    "  # tagger or POS-tagger.\n",
    "  tagged = nltk.pos_tag(wordsList)\n",
    "  print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d47394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 15, 'JJ': 6, 'CD': 6, ',': 5, 'VBD': 5, 'NNP': 3, 'IN': 2, 'NNS': 2, '.': 2, \"''\": 1, 'WDT': 1, 'POS': 1, 'JJS': 1, '$': 1, '(': 1, ')': 1, 'RB': 1, 'VBG': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter( tag for word,  tag in tagged)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7a41f",
   "metadata": {},
   "source": [
    "#### Stop Words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d296ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Besides', ',', 'teams', 'formed', 'monitor', 'progress', 'plantation', 'drive', ',', \"''\", 'said.What', 'extent', 'India', \"'s\", 'tree', 'planting', 'project', '?', 'India', 'vowed', 'third', 'total', 'land', 'area', ',', '95', 'million', 'hectares', ',', 'forest', 'tree', 'cover', '2030.The', 'government', 'allocated', '$', '6.2', 'billion', '(', '€5.2', 'billion', ')', 'tree-planting', 'across', 'country.However', ',', 'industrial', 'development', 'rapidly', 'growing', 'population', 'put', 'stress', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words = [w for w in wordsList if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58752cbc",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a92b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['besid', ',', 'team', 'form', 'monitor', 'progress', 'plantat', 'drive', ',', \"''\", 'said.what', 'extent', 'india', \"'s\", 'tree', 'plant', 'project', '?', 'india', 'vow', 'third', 'total', 'land', 'area', ',', '95', 'million', 'hectar', ',', 'forest', 'tree', 'cover', '2030.the', 'govern', 'alloc', '$', '6.2', 'billion', '(', '€5.2', 'billion', ')', 'tree-plant', 'across', 'country.howev', ',', 'industri', 'develop', 'rapidli', 'grow', 'popul', 'put', 'stress', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6699231c",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c9293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Besides', ',', 'team', 'formed', 'monitor', 'progress', 'plantation', 'drive', ',', \"''\", 'said.What', 'extent', 'India', \"'s\", 'tree', 'planting', 'project', '?', 'India', 'vowed', 'third', 'total', 'land', 'area', ',', '95', 'million', 'hectare', ',', 'forest', 'tree', 'cover', '2030.The', 'government', 'allocated', '$', '6.2', 'billion', '(', '€5.2', 'billion', ')', 'tree-planting', 'across', 'country.However', ',', 'industrial', 'development', 'rapidly', 'growing', 'population', 'put', 'stress', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e7b48",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3faed6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between 2016 and 2019, the state forest department under the BJP government had launched ‘Green Maharashtra’ drive with an aim to plant 50 crore trees across the state in the four-year period. In October 2019, the government had claimed it had surpassed the target by planting 33 crore trees in July-September 2019. The Indian Express had found that non-forest agencies — such as gram panchayats — which were tasked with planting trees had not uploaded the mandatory audio-visual proof of the tree plantation drives on the specially created portal.In Pune Revenue Division, it was claimed the gram panchayats planted 1.7 crore saplings; however, no evidence was uploaded for 87 per cent (1.49 crore) saplings. Also, out of the 59 government agencies involved in the drive as many as 38 had not submitted survival reports about the saplings.This year, the targets set by the forest department were comparatively modest. For example, Pune Circle — which comprises three divisions in Pune and Solapur district — had planned to plant 17 lakh saplings on forest land. However, it may not meet the target due to unavailability of funds. Last year Pune Circle planted 70 lakh saplings on forest land.In Pune Division — which comprises six talukas namely Maval, Mulshi, Daund, Indapur, Baramati and Havveli — preparations were done for plantation of about 4 lakh trees with special emphasis on teakwood.The National Forest Policy aims and emphasizes at maintaining 33% of the country’s geographical area under forest and green cover. In view of this and as a part of the 50 crore plantation programme within Maharashtra, the Maharashtra Forest Department aims to plant 4 crore saplings all over the State between 1st to July 7th, 2017 to celebrate ‘Vanmohotsav’. The plantation programme, which was announced in 2016 with the aim of planting 2 crore trees on 1st July 2016 was a resounding success with the final total reported figure of 2.82 crore saplings planted on a single day. To maintain consistency of this platform without affecting its momentum, the Forest Department has set the target of plantation of 4 crore, 13 crore and 33 crore saplings under the mission of 50 crore plantation which shall be accomplished in the three consecutive years viz. 2017, 2018 and 2019. The 4 crore saplings for the year 2017 will be planted during the Vanmohotsav, July 1st to July 7th in a state-wide drive with the involvement of 33 government departments along with Students of Schools and Colleges, NSS, NCC, CSR, NGOs, Railways, National Highways, Defence, NABARD and other stakeholders of Society. In a first of its kind, a 24-hour toll free helpline number 1926 called ‘Hello Forest’ has been set up to provide information regarding plantation, protection and for mass awareness. The Forest Department has created a mobile application called ‘My Plants’ to record details of the plantation such as numbers, species and location into the computer system of the Forest Department. All volunteers at individual, collective and organizational level should download and use this application to record their tree plantation work through the application, which will be operational from 1st July to 7th July.  In consonance of the public participation, the Maharashtra Forest Department has initiated the ‘Maharashtra Harit Sena’/ ‘Green Army’ which is a body of dedicated volunteers to participate in the plantation, protection, and activities in forest, wildlife, and related sectors around the year. Individuals and organisations interested in volunteering can register on the Green Army website www.greenarmy.mahaforest.gov.in An integrated drive has been set in place to ensure seamless and successful participation from all stakeholders of society, especially the public.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open('testdoc2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    text += line\n",
    "text = text.replace('\\n','')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce7d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b348eebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Between', '2016', '2019', 'state', 'forest', 'department', 'BJP', 'government', 'launched', '‘', 'Green', 'Maharashtra', '’', 'drive', 'aim', 'plant', '50', 'crore', 'trees', 'across', 'state', 'four-year', 'period', '.', 'In', 'October', '2019', 'government', 'claimed', 'surpassed', 'target', 'planting', '33', 'crore', 'trees', 'July-September', '2019', '.', 'The', 'Indian', 'Express', 'found', 'non-forest', 'agencies', '—', 'gram', 'panchayats', '—', 'tasked', 'planting', 'trees', 'uploaded', 'mandatory', 'audio-visual', 'proof', 'tree', 'plantation', 'drives', 'specially', 'created', 'portal.In', 'Pune', 'Revenue', 'Division', 'claimed', 'gram', 'panchayats', 'planted', '1.7', 'crore', 'saplings', ';', 'however', 'evidence', 'uploaded', '87', 'per', 'cent', '(', '1.49', 'crore', ')', 'saplings', '.', 'Also', '59', 'government', 'agencies', 'involved', 'drive', 'many', '38', 'submitted', 'survival', 'reports', 'saplings.This', 'year', 'targets', 'set', 'forest', 'department', 'comparatively', 'modest', '.', 'For', 'example', 'Pune', 'Circle', '—', 'comprises', 'three', 'divisions', 'Pune', 'Solapur', 'district', '—', 'planned', 'plant', '17', 'lakh', 'saplings', 'forest', 'land', '.', 'However', 'may', 'meet', 'target', 'due', 'unavailability', 'funds', '.', 'Last', 'year', 'Pune', 'Circle', 'planted', '70', 'lakh', 'saplings', 'forest', 'land.In', 'Pune', 'Division', '—', 'comprises', 'six', 'talukas', 'namely', 'Maval', 'Mulshi', 'Daund', 'Indapur', 'Baramati', 'Havveli', '—', 'preparations', 'done', 'plantation', '4', 'lakh', 'trees', 'special', 'emphasis', 'teakwood.The', 'National', 'Forest', 'Policy', 'aims', 'emphasizes', 'maintaining', '33', '%', 'country', '’', 'geographical', 'area', 'forest', 'green', 'cover', '.', 'In', 'view', 'part', '50', 'crore', 'plantation', 'programme', 'within', 'Maharashtra', 'Maharashtra', 'Forest', 'Department', 'aims', 'plant', '4', 'crore', 'saplings', 'State', '1st', 'July', '7th', '2017', 'celebrate', '‘', 'Vanmohotsav', '’', '.', 'The', 'plantation', 'programme', 'announced', '2016', 'aim', 'planting', '2', 'crore', 'trees', '1st', 'July', '2016', 'resounding', 'success', 'final', 'total', 'reported', 'figure', '2.82', 'crore', 'saplings', 'planted', 'single', 'day', '.', 'To', 'maintain', 'consistency', 'platform', 'without', 'affecting', 'momentum', 'Forest', 'Department', 'set', 'target', 'plantation', '4', 'crore', '13', 'crore', '33', 'crore', 'saplings', 'mission', '50', 'crore', 'plantation', 'shall', 'accomplished', 'three', 'consecutive', 'years', 'viz', '.', '2017', '2018', '2019', '.', 'The', '4', 'crore', 'saplings', 'year', '2017', 'planted', 'Vanmohotsav', 'July', '1st', 'July', '7th', 'state-wide', 'drive', 'involvement', '33', 'government', 'departments', 'along', 'Students', 'Schools', 'Colleges', 'NSS', 'NCC', 'CSR', 'NGOs', 'Railways', 'National', 'Highways', 'Defence', 'NABARD', 'stakeholders', 'Society', '.', 'In', 'first', 'kind', '24-hour', 'toll', 'free', 'helpline', 'number', '1926', 'called', '‘', 'Hello', 'Forest', '’', 'set', 'provide', 'information', 'regarding', 'plantation', 'protection', 'mass', 'awareness', '.', 'The', 'Forest', 'Department', 'created', 'mobile', 'application', 'called', '‘', 'My', 'Plants', '’', 'record', 'details', 'plantation', 'numbers', 'species', 'location', 'computer', 'system', 'Forest', 'Department', '.', 'All', 'volunteers', 'individual', 'collective', 'organizational', 'level', 'download', 'use', 'application', 'record', 'tree', 'plantation', 'work', 'application', 'operational', '1st', 'July', '7th', 'July', '.', 'In', 'consonance', 'public', 'participation', 'Maharashtra', 'Forest', 'Department', 'initiated', '‘', 'Maharashtra', 'Harit', 'Sena', '’', '/', '‘', 'Green', 'Army', '’', 'body', 'dedicated', 'volunteers', 'participate', 'plantation', 'protection', 'activities', 'forest', 'wildlife', 'related', 'sectors', 'around', 'year', '.', 'Individuals', 'organisations', 'interested', 'volunteering', 'register', 'Green', 'Army', 'website', 'www.greenarmy.mahaforest.gov.in', 'An', 'integrated', 'drive', 'set', 'place', 'ensure', 'seamless', 'successful', 'participation', 'stakeholders', 'society', 'especially', 'public', '.']\n"
     ]
    }
   ],
   "source": [
    "wordsList = nltk.word_tokenize(text)\n",
    "from nltk.corpus import stopwords\n",
    "words = [w for w in wordsList if w not in stopwords.words(\"english\")]\n",
    "# print(words)\n",
    "symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n—,\"\n",
    "for i in symbols:\n",
    "    data = np.char.replace(words, i, ' ')\n",
    "data = list(filter((' ').__ne__, data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e1902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Between', '2016', '2019', 'state', 'forest', 'department', 'BJP', 'government', 'launched', '‘', 'Green', 'Maharashtra', '’', 'drive', 'aim', 'plant', '50', 'crore', 'tree', 'across', 'state', 'four-year', 'period', '.', 'In', 'October', '2019', 'government', 'claimed', 'surpassed', 'target', 'planting', '33', 'crore', 'tree', 'July-September', '2019', '.', 'The', 'Indian', 'Express', 'found', 'non-forest', 'agency', '—', 'gram', 'panchayat', '—', 'tasked', 'planting', 'tree', 'uploaded', 'mandatory', 'audio-visual', 'proof', 'tree', 'plantation', 'drive', 'specially', 'created', 'portal.In', 'Pune', 'Revenue', 'Division', 'claimed', 'gram', 'panchayat', 'planted', '1.7', 'crore', 'sapling', ';', 'however', 'evidence', 'uploaded', '87', 'per', 'cent', '(', '1.49', 'crore', ')', 'sapling', '.', 'Also', '59', 'government', 'agency', 'involved', 'drive', 'many', '38', 'submitted', 'survival', 'report', 'saplings.This', 'year', 'target', 'set', 'forest', 'department', 'comparatively', 'modest', '.', 'For', 'example', 'Pune', 'Circle', '—', 'comprises', 'three', 'division', 'Pune', 'Solapur', 'district', '—', 'planned', 'plant', '17', 'lakh', 'sapling', 'forest', 'land', '.', 'However', 'may', 'meet', 'target', 'due', 'unavailability', 'fund', '.', 'Last', 'year', 'Pune', 'Circle', 'planted', '70', 'lakh', 'sapling', 'forest', 'land.In', 'Pune', 'Division', '—', 'comprises', 'six', 'talukas', 'namely', 'Maval', 'Mulshi', 'Daund', 'Indapur', 'Baramati', 'Havveli', '—', 'preparation', 'done', 'plantation', '4', 'lakh', 'tree', 'special', 'emphasis', 'teakwood.The', 'National', 'Forest', 'Policy', 'aim', 'emphasizes', 'maintaining', '33', '%', 'country', '’', 'geographical', 'area', 'forest', 'green', 'cover', '.', 'In', 'view', 'part', '50', 'crore', 'plantation', 'programme', 'within', 'Maharashtra', 'Maharashtra', 'Forest', 'Department', 'aim', 'plant', '4', 'crore', 'sapling', 'State', '1st', 'July', '7th', '2017', 'celebrate', '‘', 'Vanmohotsav', '’', '.', 'The', 'plantation', 'programme', 'announced', '2016', 'aim', 'planting', '2', 'crore', 'tree', '1st', 'July', '2016', 'resounding', 'success', 'final', 'total', 'reported', 'figure', '2.82', 'crore', 'sapling', 'planted', 'single', 'day', '.', 'To', 'maintain', 'consistency', 'platform', 'without', 'affecting', 'momentum', 'Forest', 'Department', 'set', 'target', 'plantation', '4', 'crore', '13', 'crore', '33', 'crore', 'sapling', 'mission', '50', 'crore', 'plantation', 'shall', 'accomplished', 'three', 'consecutive', 'year', 'viz', '.', '2017', '2018', '2019', '.', 'The', '4', 'crore', 'sapling', 'year', '2017', 'planted', 'Vanmohotsav', 'July', '1st', 'July', '7th', 'state-wide', 'drive', 'involvement', '33', 'government', 'department', 'along', 'Students', 'Schools', 'Colleges', 'NSS', 'NCC', 'CSR', 'NGOs', 'Railways', 'National', 'Highways', 'Defence', 'NABARD', 'stakeholder', 'Society', '.', 'In', 'first', 'kind', '24-hour', 'toll', 'free', 'helpline', 'number', '1926', 'called', '‘', 'Hello', 'Forest', '’', 'set', 'provide', 'information', 'regarding', 'plantation', 'protection', 'mass', 'awareness', '.', 'The', 'Forest', 'Department', 'created', 'mobile', 'application', 'called', '‘', 'My', 'Plants', '’', 'record', 'detail', 'plantation', 'number', 'specie', 'location', 'computer', 'system', 'Forest', 'Department', '.', 'All', 'volunteer', 'individual', 'collective', 'organizational', 'level', 'download', 'use', 'application', 'record', 'tree', 'plantation', 'work', 'application', 'operational', '1st', 'July', '7th', 'July', '.', 'In', 'consonance', 'public', 'participation', 'Maharashtra', 'Forest', 'Department', 'initiated', '‘', 'Maharashtra', 'Harit', 'Sena', '’', '/', '‘', 'Green', 'Army', '’', 'body', 'dedicated', 'volunteer', 'participate', 'plantation', 'protection', 'activity', 'forest', 'wildlife', 'related', 'sector', 'around', 'year', '.', 'Individuals', 'organisation', 'interested', 'volunteering', 'register', 'Green', 'Army', 'website', 'www.greenarmy.mahaforest.gov.in', 'An', 'integrated', 'drive', 'set', 'place', 'ensure', 'seamless', 'successful', 'participation', 'stakeholder', 'society', 'especially', 'public', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in data]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "437847de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'between': 43, '2016': 4, 'and': 30, '2019': 7, 'the': 237, 'state': 222, 'forest': 93, 'department': 70, 'under': 248, 'bjp': 44, 'government': 101, 'had': 105, 'launched': 134, 'green': 103, 'maharashtra': 138, 'drive': 78, 'with': 265, 'an': 29, 'aim': 24, 'to': 242, 'plant': 181, '50': 12, 'crore': 64, 'trees': 246, 'across': 20, 'in': 114, 'four': 95, 'year': 270, 'period': 178, 'october': 164, 'claimed': 52, 'it': 127, 'surpassed': 228, 'target': 232, 'by': 46, 'planting': 184, '33': 9, 'july': 129, 'september': 210, 'indian': 116, 'express': 88, 'found': 94, 'that': 236, 'non': 159, 'agencies': 23, 'such': 227, 'as': 36, 'gram': 102, 'panchayats': 173, 'which': 261, 'were': 260, 'tasked': 234, 'not': 160, 'uploaded': 250, 'mandatory': 141, 'audio': 38, 'visual': 254, 'proof': 191, 'of': 165, 'tree': 245, 'plantation': 182, 'drives': 79, 'on': 166, 'specially': 219, 'created': 63, 'portal': 188, 'pune': 195, 'revenue': 204, 'division': 74, 'was': 258, 'planted': 183, 'saplings': 205, 'however': 113, 'no': 158, 'evidence': 86, 'for': 92, '87': 17, 'per': 177, 'cent': 50, '49': 11, 'also': 28, 'out': 171, '59': 13, 'involved': 124, 'many': 142, '38': 10, 'submitted': 224, 'survival': 229, 'reports': 202, 'about': 18, 'this': 239, 'targets': 233, 'set': 211, 'comparatively': 55, 'modest': 149, 'example': 87, 'circle': 51, 'comprises': 56, 'three': 240, 'divisions': 75, 'solapur': 217, 'district': 73, 'planned': 180, '17': 1, 'lakh': 131, 'land': 132, 'may': 145, 'meet': 146, 'due': 80, 'unavailability': 247, 'funds': 98, 'last': 133, '70': 14, 'six': 215, 'talukas': 231, 'namely': 154, 'maval': 144, 'mulshi': 151, 'daund': 66, 'indapur': 115, 'baramati': 40, 'havveli': 108, 'preparations': 189, 'done': 76, 'special': 218, 'emphasis': 82, 'teakwood': 235, 'national': 155, 'policy': 187, 'aims': 25, 'emphasizes': 83, 'at': 37, 'maintaining': 140, 'country': 61, 'geographical': 99, 'area': 33, 'cover': 62, 'view': 253, 'part': 174, 'programme': 190, 'within': 266, 'all': 26, 'over': 172, '1st': 3, '7th': 15, '2017': 5, 'celebrate': 49, 'vanmohotsav': 252, 'announced': 31, 'resounding': 203, 'success': 225, 'final': 90, 'total': 244, 'reported': 201, 'figure': 89, '82': 16, 'single': 214, 'day': 67, 'maintain': 139, 'consistency': 59, 'platform': 186, 'without': 267, 'affecting': 22, 'its': 128, 'momentum': 150, 'has': 107, '13': 0, 'mission': 147, 'shall': 212, 'be': 41, 'accomplished': 19, 'consecutive': 58, 'years': 271, 'viz': 255, '2018': 6, 'will': 264, 'during': 81, 'wide': 262, 'involvement': 125, 'departments': 71, 'along': 27, 'students': 223, 'schools': 206, 'colleges': 54, 'nss': 161, 'ncc': 156, 'csr': 65, 'ngos': 157, 'railways': 196, 'highways': 111, 'defence': 69, 'nabard': 153, 'other': 170, 'stakeholders': 221, 'society': 216, 'first': 91, 'kind': 130, '24': 8, 'hour': 112, 'toll': 243, 'free': 96, 'helpline': 110, 'number': 162, '1926': 2, 'called': 47, 'hello': 109, 'been': 42, 'up': 249, 'provide': 193, 'information': 119, 'regarding': 198, 'protection': 192, 'mass': 143, 'awareness': 39, 'mobile': 148, 'application': 32, 'my': 152, 'plants': 185, 'record': 197, 'details': 72, 'numbers': 163, 'species': 220, 'location': 136, 'into': 123, 'computer': 57, 'system': 230, 'volunteers': 257, 'individual': 117, 'collective': 53, 'organizational': 169, 'level': 135, 'should': 213, 'download': 77, 'use': 251, 'their': 238, 'work': 268, 'through': 241, 'operational': 167, 'from': 97, 'consonance': 60, 'public': 194, 'participation': 176, 'initiated': 120, 'harit': 106, 'sena': 209, 'army': 34, 'is': 126, 'body': 45, 'dedicated': 68, 'participate': 175, 'activities': 21, 'wildlife': 263, 'related': 200, 'sectors': 208, 'around': 35, 'individuals': 118, 'organisations': 168, 'interested': 122, 'volunteering': 256, 'can': 48, 'register': 199, 'website': 259, 'www': 269, 'greenarmy': 104, 'mahaforest': 137, 'gov': 100, 'integrated': 121, 'place': 179, 'ensure': 84, 'seamless': 207, 'successful': 226, 'especially': 85}\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>17</th>\n",
       "      <th>1926</th>\n",
       "      <th>1st</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>24</th>\n",
       "      <th>33</th>\n",
       "      <th>...</th>\n",
       "      <th>wide</th>\n",
       "      <th>wildlife</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>within</th>\n",
       "      <th>without</th>\n",
       "      <th>work</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420109</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.191336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253925</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.104567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096209</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135387</td>\n",
       "      <td>0.331415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.177609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.126708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505696</td>\n",
       "      <td>0.645676</td>\n",
       "      <td>0.505696</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146575</td>\n",
       "      <td>0.220365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.141636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          13        17     1926       1st      2016      2017      2018  \\\n",
       "0   0.000000  0.000000  0.00000  0.000000  0.193158  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.253925  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.00000  0.150327  0.000000  0.165033  0.000000   \n",
       "8   0.000000  0.000000  0.00000  0.135387  0.331415  0.000000  0.000000   \n",
       "9   0.177609  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.00000  0.000000  0.000000  0.505696  0.645676   \n",
       "11  0.000000  0.000000  0.00000  0.119755  0.000000  0.131471  0.000000   \n",
       "12  0.000000  0.000000  0.21529  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.00000  0.141636  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        2019       24        33  ...      wide  wildlife      will      with  \\\n",
       "0   0.173253  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.145200   \n",
       "1   0.420109  0.00000  0.191336  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.088626   \n",
       "3   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.00000  0.104567  ...  0.000000  0.000000  0.000000  0.096209   \n",
       "7   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.249131   \n",
       "9   0.000000  0.00000  0.126708  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.505696  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.00000  0.119755  ...  0.167863  0.000000  0.146575  0.220365   \n",
       "12  0.000000  0.21529  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.173356  0.000000   \n",
       "15  0.000000  0.00000  0.000000  ...  0.000000  0.186304  0.000000  0.000000   \n",
       "16  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      within   without      work       www      year     years  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.145200  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.131121  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.096209  0.000000  \n",
       "7   0.210715  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.177609  0.000000  0.000000  0.000000  0.177609  \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.110183  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.000000  0.000000  0.198534  0.000000  0.000000  0.000000  \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.122287  0.000000  \n",
       "16  0.000000  0.000000  0.000000  0.185441  0.000000  0.000000  \n",
       "\n",
       "[17 rows x 272 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = sent_tokenize(text)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf_tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "df_tfidfvect = pd.DataFrame(data = X.toarray(),columns = tfidf_tokens)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc8762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
